{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultivariateStatistical SMTP Projection Methods.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRspQudEnBea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxRPTqlOnKCv",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "ca230cea-2835-49de-8392-6a5b9e8d54ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#@title Default title text\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbEYNARmnLqb",
        "colab_type": "text"
      },
      "source": [
        "# Average AR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uMKLLD_nSia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "import sys\n",
        "\n",
        "class AR_Compact:\n",
        "\n",
        "    def model_persistence(self, x):\n",
        "        return x\n",
        "\n",
        "    def create_persistence(self):\n",
        "        predictions = list()\n",
        "        for x in self.test_X:\n",
        "            yhat = self.model_persistence(x)\n",
        "            predictions.append(yhat)\n",
        "        rmse = sqrt(mean_squared_error(self.test_y, predictions))\n",
        "        # print('Train shape', self.train_X.shape, ' - Test shape:' , self.test_X.shape)\n",
        "        # print('Persistent Model RMSE: %.3f' % rmse)     \n",
        "        \n",
        "\n",
        "    @classmethod\n",
        "    def from_DataFrame(cls,dataframe, train_rate) -> 'AR_Compact':\n",
        "    \treturn cls(dataframe, train_rate)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, file: str, train_rate) -> 'AR_Compact':\n",
        "    \tdf = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    \treturn cls(df, train_rate)\n",
        "\n",
        "    \n",
        "    def __init__(self,df, train_rate):\n",
        "        self.df = df\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "        self.df.loc[self.df.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "        self.df.loc[self.df.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "        self.train_size = int(len(self.values) * train_rate)\n",
        "\n",
        "        self.train, self.test = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = self.train[:,0], self.train[:,1]\n",
        "        self.test_X, self.test_y = self.test[:,0], self.test[:,1]     \n",
        "        # self.create_persistence()\n",
        "\n",
        "        # X = (self.dataframe['t+1'] - self.dataframe['t']).values\n",
        "        X = series.values\n",
        "\n",
        "        self.train, self.test = X[1:self.train_size], X[self.train_size:]\n",
        "\n",
        "          \n",
        "    def fit(self, verbose=False):\n",
        "        self.model = AR(self.train)\n",
        "        self.model_fit = self.model.fit()\n",
        "        self.window = self.model_fit.k_ar\n",
        "        self.coef = self.model_fit.params  \n",
        "        if verbose:      \n",
        "            print('Lag: %s' % self.model_fit.k_ar)\n",
        "            print('Coefficients: %s' % self.model_fit.params)\n",
        "\n",
        "    \n",
        "    def predict(self):\n",
        "        self.history = self.train[len(self.train)-self.window:]\n",
        "        self.history = [self.history[i] for i in range(len(self.history))]\n",
        "        self.predictions = list()\n",
        "        for t in range(len(self.test)):\n",
        "            length = len(self.history)\n",
        "            lag = [self.history[i] for i in range(length-self.window,length)]\n",
        "            yhat = self.coef[0]\n",
        "            for d in range(self.window):\n",
        "                yhat += self.coef[d+1] * lag[self.window-d-1]\n",
        "            obs = self.test[t]\n",
        "            self.predictions.append(yhat)\n",
        "            self.history.append(obs)        \n",
        "        # for i in range(len(predictions)):\n",
        "        #     print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
        "        # rmse = sqrt(mean_squared_error(self.test, self.predictions))\n",
        "        self.errors = np.absolute(self.test - np.array(self.predictions))\n",
        "        # print('Prediction Test RMSE: %.3f' % rmse)\n",
        "    def plot(self):\n",
        "        # plot predicted error\n",
        "        df_test = pd.DataFrame(self.df.iloc[self.train_size:].values)\n",
        "\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test,color ='blue', linewidth=0.5)\n",
        "        pyplot.plot(self.predictions, color='green',  linewidth=0.5)\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5)\n",
        "        pyplot.plot(df_test[df_test[1]==1.].index, df_test[df_test[1]==1.].iloc[:,0].values,'ro')\n",
        "        pyplot.show()\n",
        "\n",
        "    # def plot(self):\n",
        "    #     # plot predicted error\n",
        "    #     indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "    #     pyplot.figure(figsize=(50,5))\n",
        "    #     pyplot.plot(self.test, color='green',  linewidth=0.5,label='True Values')\n",
        "    #     pyplot.plot(self.predictions, color='blue',  linewidth=0.5,label='Predictions')\n",
        "    #     pyplot.plot(self.errors, color = 'red',  linewidth=0.5, label='Errors')\n",
        "    #     pyplot.plot(self.df[self.df['is_anomaly']==1].index[indices] - self.train_size, self.test[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size -1], linestyle=\"\",marker=\".\", label='Anomalies')\n",
        "    #     pyplot.legend()\n",
        "    #     pyplot.show()\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, max(true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended),np.max(self.errors) + np.mean(self.errors))]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "\n",
        "# ar_model = AR_Compact('drive/My Drive/MT/Experiments/Univariate/YahooServiceNetworkTraffic/A4Benchmark/A4Benchmark-TS18.csv', 0.3)\n",
        "# ar_model.fit()\n",
        "# ar_model.predict()\n",
        "# ar_model.plot()\n",
        "# ar_model.get_roc_auc(verbose=True)\n",
        "\n",
        "def concatenate_errors(ar_univariates, dimension):\n",
        "    pci_ = ar_univariates[0]\n",
        "    errors = np.zeros((len(pci_.errors),dimension))\n",
        "    errors[:,0] = pci_.errors.T\n",
        "    # for i in range(1,5):\n",
        "    for i in range(1,dimension):\n",
        "        errors[:,0:i+1] = np.c_[errors[:,:i],ar_univariates[i].errors.reshape(-1,1)]\n",
        "    return np.min(errors,axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fga-LRqOnV11",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGb80DtanW4Q",
        "colab_type": "code",
        "outputId": "57e041a4-2be0-4382-8f24-56610bf0db40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "\n",
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "dimension = 5\n",
        "# df = read_csv(path, header=0, index_col=index_col, parse_dates=True,squeeze=True)\n",
        "ar_univariates = []\n",
        "for i in range (5):\n",
        "    print(i)\n",
        "    df_univariate = pd.DataFrame(np.c_[df_synthetic.iloc[:,i].values,df_synthetic.iloc[:,-1].values])\n",
        "    df_univariate.columns = ['V1','is_anomaly']\n",
        "    ar = AR_Compact.from_DataFrame(df_univariate,0.3)\n",
        "    # ar.getAndReadAnaomaliesByPCI(plot=False)\n",
        "    ar.fit()\n",
        "    ar.predict()\n",
        "    ar_univariates.append(ar)\n",
        "\t\n",
        "errors = concatenate_errors(ar_univariates)\n",
        "ar_full = AR_Compact.from_DataFrame(df_synthetic,0.3)\n",
        "ar_full.errors = errors\n",
        "ar_full.get_roc_auc(verbose=False)\n",
        "\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff.microseconds/1000, 'ms')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2aa40bb016ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf_univariate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_synthetic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_synthetic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdf_univariate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'V1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'is_anomaly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAR_Compact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_DataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_univariate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_synthetic' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZdg3G4BnZoG",
        "colab_type": "code",
        "outputId": "03e38cde-f89b-4ba7-ae1c-0454d04bf5f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "def concatenate_errors(ar_univariates, dimension):\n",
        "    pci_ = ar_univariates[0]\n",
        "    errors = np.zeros((len(pci_.errors),dimension))\n",
        "    errors[:,0] = pci_.errors.T\n",
        "    # for i in range(1,5):\n",
        "    for i in range(1,dimension):\n",
        "        errors[:,0:i+1] = np.c_[errors[:,:i],ar_univariates[i].errors.reshape(-1,1)]\n",
        "    return np.max(errors,axis=1)\n",
        "\n",
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "dimension = 2\n",
        "# df = read_csv(path, header=0, index_col=index_col, parse_dates=True,squeeze=True)\n",
        "ar_univariates = []\n",
        "df_stmp = read_csv('drive/My Drive/MT/Experiments/Multivariate/NASA_Shuttle/40903.csv', header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "df_stmp.rename(columns={'Target':'is_anomaly'}, inplace=True)\n",
        "df_stmp.loc[df_stmp.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "df_stmp.loc[df_stmp.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "\n",
        "for i in range (dimension):\n",
        "    print(i)\n",
        "    df_univariate = pd.DataFrame(np.c_[df_stmp.iloc[:,i].values,df_stmp.iloc[:,-1].values])\n",
        "    df_univariate.columns = ['V1','is_anomaly']\n",
        "    ar = AR_Compact.from_DataFrame(df_univariate,0.3)\n",
        "    # ar.getAndReadAnaomaliesByPCI(plot=False)\n",
        "    ar.fit()\n",
        "    ar.predict()\n",
        "    ar_univariates.append(ar)\n",
        "\t\n",
        "errors = concatenate_errors(ar_univariates,dimension)\n",
        "ar_full = AR_Compact.from_DataFrame(df_stmp,0.3)\n",
        "ar_full.errors = errors\n",
        "ar_full.get_roc_auc(verbose=False)\n",
        "\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  result = method(y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKPBFRLKEAl9EJEv8f8gzbeZSCms9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.5490260471400693\n",
            "Time:  0:00:24.144307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNZmWEq0b0Dj",
        "colab_type": "text"
      },
      "source": [
        "# Average MA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq2cY8Urb1p6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "import sys\n",
        "\n",
        "class MA_Average:\n",
        "\n",
        "    @classmethod\n",
        "    def from_DataFrame(cls,dataframe, train_rate) -> 'MA':\n",
        "    \treturn cls(dataframe, train_rate)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, file: str, train_rate) -> 'MA':\n",
        "    \tdf = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    \treturn cls(df, train_rate)\n",
        "\n",
        "    \n",
        "    def __init__(self,df, train_rate):\n",
        "        self.df = df\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "        self.df.loc[self.df.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "        self.df.loc[self.df.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "        X = self.dataframe.values\n",
        "        self.train_size = int(len(X) * train_rate)\n",
        "\n",
        "        self.train, self.test = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = self.train[:,0], self.train[:,1]\n",
        "        self.test_X, self.test_y = self.test[:,0], self.test[:,1]     \n",
        "        # self.create_persistence()\n",
        "\n",
        "        # X = (self.dataframe['t+1'] - self.dataframe['t']).values\n",
        "        X = series.values\n",
        "\n",
        "        # persistence model on training set\n",
        "        self.train_pred = [x for x in self.train_X]\n",
        "        # calculate residuals\n",
        "        self.train_resid = [self.train_y[i]-self.train_pred[i] for i in range(len(self.train_pred))]\n",
        "\n",
        "    # def __init__(self, path, train_rate):\n",
        "\n",
        "    #     self.df = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    #     self.df = self.df.reset_index(drop=True)\n",
        "    #     self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "    #     series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "    #     self.values = DataFrame(series.values)\n",
        "    #     self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "    #     self.dataframe.columns = ['t', 't+1']\n",
        "    #     X = self.dataframe.values\n",
        "\n",
        "    #     self.train_size = int(len(X) * train_rate)    \n",
        "\n",
        "    #     train, test = X[1:self.train_size], X[self.train_size:]\n",
        "    #     self.train_X, self.train_y = train[:,0], train[:,1]\n",
        "    #     self.test_X, self.test_y = test[:,0], test[:,1]        \n",
        "    #     # persistence model on training set\n",
        "    #     self.train_pred = [x for x in self.train_X]\n",
        "    #     # calculate residuals\n",
        "    #     self.train_resid = [self.train_y[i]-self.train_pred[i] for i in range(len(self.train_pred))]\n",
        "\n",
        "    def fit(self, verbose=False):\n",
        "        self.model = AR(self.train_resid)\n",
        "        self.model_fit = self.model.fit()\n",
        "        self.window = self.model_fit.k_ar\n",
        "        self.coef = self.model_fit.params        \n",
        "        if verbose:\n",
        "            print(self.coef)\n",
        "\n",
        "    def predict(self):\n",
        "        # walk forward over time steps in test\n",
        "        self.history = self.train_resid[len(self.train_resid)-self.window:]\n",
        "        self.history = [self.history[i] for i in range(len(self.history))]\n",
        "        self.predictions = list()\n",
        "        for t in range(len(self.test_y)):\n",
        "            # persistence\n",
        "            yhat = self.test_X[t]\n",
        "            error = self.test_y[t] - yhat\n",
        "            # predict error\n",
        "            length = len(self.history)\n",
        "            lag = [self.history[i] for i in range(length-self.window,length)]\n",
        "            pred_error = self.coef[0]\n",
        "            for d in range(self.window):\n",
        "                pred_error += self.coef[d+1] * lag[self.window-d-1]\n",
        "            # correct the prediction\n",
        "            yhat = yhat + pred_error\n",
        "            self.predictions.append(yhat)\n",
        "            self.history.append(error)\n",
        "            # print('predicted=%f, expected=%f' % (yhat, test_y[t]))\n",
        "        rmse = sqrt(mean_squared_error(self.test_y, self.predictions))\n",
        "        self.errors = np.absolute(self.test_y - np.array(self.predictions))\n",
        "        # print('Prediction Test RMSE: %.3f' % rmse)\n",
        "\n",
        "    def plot(self):\n",
        "        # plot predicted error\n",
        "        df_test = pd.DataFrame(self.df.iloc[self.train_size:].values)\n",
        "\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test_y,color ='blue', linewidth=0.5)\n",
        "        pyplot.plot(self.predictions, color='green',  linewidth=0.5)\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5)\n",
        "        pyplot.plot(df_test[df_test[1]==1.].index, df_test[df_test[1]==1.].iloc[:,0].values,'ro')\n",
        "        pyplot.show()\n",
        "\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size ]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index - self.train_size ]\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended)]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "\n",
        "\n",
        "# ar_model = AR_Compact('drive/My Drive/MT/Experiments/Univariate/YahooServiceNetworkTraffic/A4Benchmark/A4Benchmark-TS18.csv', 0.3)\n",
        "# ar_model.fit()\n",
        "# ar_model.predict()\n",
        "# ar_model.plot()\n",
        "# ar_model.get_roc_auc(verbose=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn-XLG-7cCUx",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCGN9poEcDEE",
        "colab_type": "code",
        "outputId": "cf52e333-22b8-4e1b-8a45-b5181a1e794b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "\n",
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "def concatenate_errors(ar_univariates, dimension):\n",
        "    pci_ = ar_univariates[0]\n",
        "    errors = np.zeros((len(pci_.errors),dimension))\n",
        "    errors[:,0] = pci_.errors.T\n",
        "    # for i in range(1,5):\n",
        "    for i in range(1,dimension):\n",
        "        errors[:,0:i+1] = np.c_[errors[:,:i],ar_univariates[i].errors.reshape(-1,1)]\n",
        "    return np.mean(errors,axis=1)\n",
        "\n",
        "dimension = 2\n",
        "# df = read_csv(path, header=0, index_col=index_col, parse_dates=True,squeeze=True)\n",
        "ar_univariates = []\n",
        "df_stmp = read_csv('drive/My Drive/MT/Experiments/Multivariate/NASA_Shuttle/40903.csv', header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "df_stmp.rename(columns={'Target':'is_anomaly'}, inplace=True)\n",
        "df_stmp.loc[df_stmp.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "df_stmp.loc[df_stmp.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "\n",
        "for i in range (dimension):\n",
        "    print(i)\n",
        "    df_univariate = pd.DataFrame(np.c_[df_stmp.iloc[:,i].values,df_stmp.iloc[:,-1].values])\n",
        "    df_univariate.columns = ['V1','is_anomaly']\n",
        "    ar = MA_Average.from_DataFrame(df_univariate,0.3)\n",
        "    # ar.getAndReadAnaomaliesByPCI(plot=False)\n",
        "    ar.fit()\n",
        "    ar.predict()\n",
        "    ar_univariates.append(ar)\n",
        "\t\n",
        "errors = concatenate_errors(ar_univariates,dimension)\n",
        "ar_full = MA_Average.from_DataFrame(df_stmp,0.3)\n",
        "ar_full.errors = errors\n",
        "ar_full.get_roc_auc(verbose=False)\n",
        "\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  result = method(y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  result = method(y)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  result = method(y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLyKSIf4Tm3qSy72eZuoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.5783694265125358\n",
            "Time:  0:00:04.795467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH5gdEMvcgpg",
        "colab_type": "text"
      },
      "source": [
        "# Average XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiPDp-3FciPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "\n",
        "class XGBRegressor_Average_AnomalyDetection:\n",
        "\n",
        "    @classmethod\n",
        "    def from_DataFrame(cls,dataframe,window_width, train_rate) -> 'XGBRegressor_AnomalyDetection':\n",
        "    \treturn cls(dataframe, window_width, train_rate)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, path, window_width, train_rate) -> 'XGBRegressor_AnomalyDetection':\n",
        "    \tdf = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    \treturn cls(df,window_width, train_rate)\n",
        "         \n",
        "    def __init__(self,df, window_width, train_rate):\n",
        "        self.df = df\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "        self.window_width = window_width\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "        self.train_size = int(len(self.values) * train_rate)\n",
        "\n",
        "        # train_labeled, test_labeled = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        # self.train_X, self.train_y = train_labeled[:,0], train_labeled[:,1]\n",
        "        # self.test_X, self.test_y = test_labeled[:,0], test_labeled[:,1]     \n",
        "        # self.create_persistence()\n",
        "\n",
        "        # X = series.values\n",
        "        # self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "\n",
        "    def __build_sets(self):\n",
        "        train_labeled, test_labeled = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = train_labeled[:,0], train_labeled[:,1]\n",
        "        self.test_X, self.test_y = test_labeled[:,0], test_labeled[:,1]   \n",
        "\n",
        "        X = self.dataframe.iloc[:,1].values\n",
        "        self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "\n",
        "    def standardize_dataframe(self):\n",
        "        X = self.dataframe.values\n",
        "        self.scalar = preprocessing.StandardScaler().fit(X)\n",
        "        X = self.scalar.transform(X)\n",
        "        self.dataframe = pd.DataFrame(X)\n",
        "\n",
        "    def inverse_standardize_dataframe(self):\n",
        "        X = self.dataframe.values\n",
        "        X = self.scalar.inverse_transform(X)\n",
        "        self.dataframe = pd.DataFrame(X)\n",
        "\n",
        "\n",
        "\n",
        "    def model_persistence(self, x):\n",
        "        return x\n",
        "        \n",
        "    def create_persistence(self):\n",
        "        rmse = sqrt(mean_squared_error(self.dataframe['t'].iloc[self.train_size:], self.dataframe['t+1'].iloc[self.train_size::]))\n",
        "#         print('Persistent Model RMSE: %.3f' % rmse)   \n",
        "\n",
        "    def fit(self):\n",
        "        self.create_persistence()\n",
        "        self.__build_sets()\n",
        "                \n",
        "        self.compute_anomalyScores()\n",
        "    \n",
        "    def getWindowedVectors(self, X):\n",
        "        vectors = []\n",
        "        for i,_ in enumerate(X[:-self.window_width+1]):\n",
        "            vectors.append(X[i:i+self.window_width])\n",
        "        return vectors\n",
        "\n",
        "    def compute_anomalyScores(self):\n",
        "\n",
        "        xgb = XGBRegressor()\n",
        "        xgb.fit(self.train_X.reshape(-1,1),self.train_y.reshape(-1,1))\n",
        "\n",
        "        self.predictions = xgb.predict(self.test_X.reshape(-1,1))\n",
        "        rmse = sqrt(mean_squared_error(self.test, self.predictions))\n",
        "        self.errors = np.absolute(self.test - np.array(self.predictions))\n",
        "#         print('Prediction Test RMSE: %.3f' % rmse)\n",
        "    \n",
        "\n",
        "    def plot(self):\n",
        "        # plot predicted error\n",
        "        df_test = pd.DataFrame(self.df.iloc[self.train_size:].values)\n",
        "\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test)\n",
        "        pyplot.plot(self.predictions, color='blue')\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5)\n",
        "        pyplot.plot(df_test[df_test[1]==1.].index, df_test[df_test[1]==1.].iloc[:,0].values,'ro')\n",
        "        pyplot.show()\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size ]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended)]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "\n",
        "# ar_model = AR_Compact('drive/My Drive/MT/Experiments/Univariate/YahooServiceNetworkTraffic/A4Benchmark/A4Benchmark-TS18.csv', 0.3)\n",
        "# ar_model.fit()\n",
        "# ar_model.predict()\n",
        "# ar_model.plot()\n",
        "# ar_model.get_roc_auc(verbose=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aopUoTsZcnUZ",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbERrQZccoI_",
        "colab_type": "code",
        "outputId": "5243fade-a4ea-42b7-c977-320c7287ee71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "def concatenate_errors(ar_univariates, dimension):\n",
        "    pci_ = ar_univariates[0]\n",
        "    errors = np.zeros((len(pci_.errors),dimension))\n",
        "    errors[:,0] = pci_.errors.T\n",
        "    # for i in range(1,5):\n",
        "    for i in range(1,dimension):\n",
        "        errors[:,0:i+1] = np.c_[errors[:,:i],ar_univariates[i].errors.reshape(-1,1)]\n",
        "    return np.mean(errors,axis=1)\n",
        "\n",
        "\n",
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "dimension = 2\n",
        "# df = read_csv(path, header=0, index_col=index_col, parse_dates=True,squeeze=True)\n",
        "ar_univariates = []\n",
        "df_stmp = read_csv('drive/My Drive/MT/Experiments/Multivariate/NASA_Shuttle/40903.csv', header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "df_stmp.rename(columns={'Target':'is_anomaly'}, inplace=True)\n",
        "df_stmp.loc[df_stmp.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "df_stmp.loc[df_stmp.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "\n",
        "for i in range (dimension):\n",
        "    print(i)\n",
        "    df_univariate = pd.DataFrame(np.c_[df_stmp.iloc[:,i].values,df_stmp.iloc[:,-1].values])\n",
        "    df_univariate.columns = ['V1','is_anomaly']\n",
        "    ar = XGBRegressor_Average_AnomalyDetection.from_DataFrame(df_univariate,100,0.3)\n",
        "    # ar.getAndReadAnaomaliesByPCI(plot=False)\n",
        "    ar.fit()\n",
        "    ar_univariates.append(ar)\n",
        "\t\n",
        "errors = concatenate_errors(ar_univariates,dimension)\n",
        "ar_full = XGBRegressor_Average_AnomalyDetection.from_DataFrame(df_stmp,100,0.3)\n",
        "ar_full.errors = errors\n",
        "ar_full.get_roc_auc(verbose=False)\n",
        "\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[12:05:10] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "1\n",
            "[12:05:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKKHAFxEZJRT4IiKjxP8Hv/79CDQPOyAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.5747091277585948\n",
            "Time:  0:00:01.697473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC9piKPkc9DJ",
        "colab_type": "text"
      },
      "source": [
        "# Average OCSVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS8-ZCegc-yP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "\n",
        "class OneClassSVM_Average_AnomalyDetection:\n",
        "\n",
        "    @classmethod\n",
        "    def from_DataFrame(cls,dataframe,window_width, nu, train_rate) -> 'OneClassSVM_AnomalyDetection':\n",
        "    \treturn cls(dataframe, window_width, nu, train_rate)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, path, window_width, nu, train_rate) -> 'OneClassSVM_AnomalyDetection':\n",
        "    \tdf = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    \treturn cls(df,window_width, nu, train_rate)\n",
        "     \n",
        "    def __init__(self,df, window_width, nu, train_rate):\n",
        "        self.df = df\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "        self.nu = nu\n",
        "        self.window_width = window_width\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "        self.train_size = int(len(self.values) * train_rate)\n",
        "\n",
        "        # train_labeled, test_labeled = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        # self.train_X, self.train_y = train_labeled[:,0], train_labeled[:,1]\n",
        "        # self.test_X, self.test_y = test_labeled[:,0], test_labeled[:,1]     \n",
        "        # self.create_persistence()\n",
        "\n",
        "        # X = series.values\n",
        "        # self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "\n",
        "    def __build_sets(self):\n",
        "        train_labeled, test_labeled = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = train_labeled[:,0], train_labeled[:,1]\n",
        "        self.test_X, self.test_y = test_labeled[:,0], test_labeled[:,1]   \n",
        "\n",
        "        X = self.dataframe.iloc[:,1].values\n",
        "        self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "\n",
        "    def standardize_dataframe(self):\n",
        "        X = self.dataframe.values\n",
        "        self.scalar = preprocessing.StandardScaler().fit(X)\n",
        "        X = self.scalar.transform(X)\n",
        "        self.dataframe = pd.DataFrame(X)\n",
        "\n",
        "    def inverse_standardize_dataframe(self):\n",
        "        X = self.dataframe.values\n",
        "        X = self.scalar.inverse_transform(X)\n",
        "        self.dataframe = pd.DataFrame(X)\n",
        "\n",
        "\n",
        "\n",
        "    def model_persistence(self, x):\n",
        "        return x\n",
        "        \n",
        "    def create_persistence(self):\n",
        "        rmse = sqrt(mean_squared_error(self.dataframe['t'].iloc[self.train_size:], self.dataframe['t+1'].iloc[self.train_size::]))\n",
        "#         print('Persistent Model RMSE: %.3f' % rmse)   \n",
        "\n",
        "    def fit(self):\n",
        "        self.create_persistence()\n",
        "        self.standardize_dataframe()\n",
        "        self.__build_sets()\n",
        "                \n",
        "        self.compute_anomalyScores()\n",
        "        self.inverse_standardize_dataframe()\n",
        "    \n",
        "    def getWindowedVectors(self, X):\n",
        "        vectors = []\n",
        "        for i,_ in enumerate(X[:-self.window_width+1]):\n",
        "            vectors.append(X[i:i+self.window_width])\n",
        "        return vectors\n",
        "\n",
        "    def compute_anomalyScores(self):\n",
        "        self.errors = np.zeros_like(self.test)\n",
        "        # compute anomalies\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "        # history = self.getWindowedVectors(self.train)\n",
        "\n",
        "        for i,_ in enumerate(self.test[:-self.window_width+1]):\n",
        "            sys.stdout.write('\\r'+str(i)+':'+str(len(self.test) - self.window_width))\n",
        "\n",
        "            window = self.test[i:i+self.window_width]\n",
        "            window2D = np.zeros((len(window),2))\n",
        "            window2D[:,1] = window\n",
        "            clf=OneClassSVM(nu=self.nu)\n",
        "            clf.fit(window2D)\n",
        "            error = clf.decision_function(window2D) \n",
        "            error[error>0] = 0\n",
        "            self.errors[i:i+self.window_width] += error*-10\n",
        "\n",
        "\n",
        "        # normalize anomaly score\n",
        "        self.errors[:-self.window_width+1] /= self.window_width\n",
        "        for i,error in enumerate(self.test[-self.window_width+1:]):\n",
        "            self.errors[-self.window_width + 1 + i] /=self.window_width-(i+1)\n",
        "\n",
        "        # self.errors_original = self.errors\n",
        "        # scalar = preprocessing.MinMaxScaler((0,1)).fit(self.errors.reshape(-1,1))\n",
        "        # self.errors = scalar.transform(self.errors.reshape(-1,1))*10\n",
        "\n",
        "\n",
        "    def plot(self):\n",
        "        df_test = pd.DataFrame(self.df.iloc[self.train_size:].values)\n",
        "\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test)\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5)\n",
        "        pyplot.plot(df_test[df_test[1]==1.].index, df_test[df_test[1]==1.].iloc[:,0].values,'ro')\n",
        "        pyplot.show()\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size ]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended)]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "\n",
        "\n",
        "# iforest = OneClassSVM_AnomalyDetection.from_file('drive/My Drive/MT/Experiments/Univariate/YahooServiceNetworkTraffic/A1Benchmark/real_1.csv',30,0.7,0.3)\n",
        "# iforest.fit()\n",
        "# iforest.plot()\n",
        "# iforest.get_roc_auc(verbose=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO7zvQIJdB4A",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQFH-NKmdDK_",
        "colab_type": "code",
        "outputId": "eed12660-11de-4f4f-adbb-d410955486cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def concatenate_errors(ar_univariates, dimension):\n",
        "    pci_ = ar_univariates[0]\n",
        "    errors = np.zeros((len(pci_.errors),dimension))\n",
        "    errors[:,0] = pci_.errors.T\n",
        "    # for i in range(1,5):\n",
        "    for i in range(1,dimension):\n",
        "        errors[:,0:i+1] = np.c_[errors[:,:i],ar_univariates[i].errors.reshape(-1,1)]\n",
        "    return np.mean(errors,axis=1)\n",
        "\n",
        "\n",
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "dimension = 2\n",
        "# df = read_csv(path, header=0, index_col=index_col, parse_dates=True,squeeze=True)\n",
        "ar_univariates = []\n",
        "df_stmp = read_csv('drive/My Drive/MT/Experiments/Multivariate/NASA_Shuttle/40903.csv', header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "df_stmp.rename(columns={'Target':'is_anomaly'}, inplace=True)\n",
        "df_stmp.loc[df_stmp.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "df_stmp.loc[df_stmp.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "\n",
        "# df = read_csv(path, header=0, index_col=index_col, parse_dates=True,squeeze=True)\n",
        "ar_univariates = []\n",
        "for i in range (dimension):\n",
        "    print(i)\n",
        "    df_univariate = pd.DataFrame(np.c_[df_stmp.iloc[:,i].values,df_stmp.iloc[:,-1].values])\n",
        "    df_univariate.columns = ['V1','is_anomaly']\n",
        "    ar = OneClassSVM_Average_AnomalyDetection.from_DataFrame(df_univariate,30,0.7,0.3)\n",
        "    # ar.getAndReadAnaomaliesByPCI(plot=False)\n",
        "    ar.fit()\n",
        "    ar_univariates.append(ar)\n",
        "\t\n",
        "errors = concatenate_errors(ar_univariates,dimension)\n",
        "ar_full = OneClassSVM_Average_AnomalyDetection.from_DataFrame(df_stmp,30,0.7,0.3)\n",
        "ar_full.errors = errors\n",
        "ar_full.get_roc_auc(verbose=False)\n",
        "\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff.total_seconds(), 'ms')\n",
        "\n",
        "# iforest = XGBRegressor_AnomalyDetection('Univariate/YahooServiceNetworkTraffic/A1Benchmark/real_4.csv',30,0.7,0.66)\n",
        "# iforest.fit()\n",
        "# iforest.plot()\n",
        "# iforest.get_roc_auc(verbose=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "66580:665801\n",
            "66580:66580"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.7670920282239904\n",
            "Time:  103.846792 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCnfXsL3iJub",
        "colab_type": "text"
      },
      "source": [
        "# Projection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usmPGrVViK9j",
        "colab_type": "code",
        "outputId": "c888947f-3282-4e6f-a792-229d30d3caa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(42)\n",
        "from numpy.random import seed\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "seed(42)\n",
        "from keras import regularizers\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "    \n",
        "class Projection_AnomalyDetection:\n",
        "\n",
        "    @classmethod\n",
        "    def from_DataFrame(cls,dataframe,window_width, dimension, train_rate) -> 'Projection_AnomalyDetection':\n",
        "    \treturn cls(dataframe, window_width, dimension, train_rate)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, path, window_width, dimension, train_rate) -> 'Projection_AnomalyDetection':\n",
        "    \tdf = read_csv(path, header=0, index_col=None, parse_dates=True,squeeze=True)\n",
        "    \treturn cls(df,window_width, dimension, train_rate)\n",
        "         \n",
        "    # def __init__(self,path, window_width, dimension, train_rate):\n",
        "\n",
        "    #     self.dimension = dimension\n",
        "    #     self.n_epochs = n_epochs\n",
        "    #     self.window_width = window_width\n",
        "        \n",
        "    #     self.n_filters = n_filters\n",
        "    #     self.kernel_size = kernel_size\n",
        "    #     self.n_dense = n_dense\n",
        "\n",
        "    #     self.df = read_csv(path, header=0, index_col=None, parse_dates=True,squeeze=True)\n",
        "    #     self.df = self.df.reset_index(drop=True)\n",
        "    #     self.df.rename(columns={'Target':'is_anomaly'}, inplace=True)\n",
        "    #     self.df.loc[self.df.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "    #     self.df.loc[self.df.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "        \n",
        "    #     self.X_origin = self.df.iloc[:,:dimension].values\n",
        "    #     self.Y_origin = self.df.iloc[:,-1].values\n",
        "\n",
        "    #     df_sensors = pd.DataFrame(self.df.iloc[:,:dimension].values)  \n",
        "    #     self.values = df_sensors\n",
        "    #     self.dataframe = concat([self.df.iloc[:,:-1].shift(1), self.df.iloc[:,:-1], self.df.iloc[:,-1]], axis=1)\n",
        "    #     self.dataframe.columns = np.r_[np.array(['V'+str(i)+'_t' for i in range(1,10)]),np.array(['V'+str(i)+'_t+1' for i in range(1,10)]),['is_anomaly']]\n",
        "\n",
        "    #     # self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "    #     # self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "    #     self.train_size = int(len(self.values) * train_rate)  \n",
        "\n",
        "    def __init__(self,dataframe, window_width, dimension, train_rate):\n",
        "\n",
        "        self.df = dataframe\n",
        "        self.dimension = dimension\n",
        "        self.window_width = window_width\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'Target':'is_anomaly'}, inplace=True)\n",
        "        self.df.loc[self.df.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "        self.df.loc[self.df.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "\n",
        "        self.X_origin = self.df.iloc[:,:dimension].values\n",
        "        self.Y_origin = self.df.iloc[:,-1].values\n",
        "\n",
        "        df_sensors = pd.DataFrame(self.df.iloc[:,:dimension].values)  \n",
        "        self.values = df_sensors\n",
        "        self.dataframe = concat([self.df.iloc[:,:-1].shift(1), self.df.iloc[:,:-1], self.df.iloc[:,-1]], axis=1)\n",
        "        self.dataframe.columns = np.r_[np.array(['V'+str(i)+'_t' for i in range(1,self.dimension+1)]),np.array(['V'+str(i)+'_t+1' for i in range(1,self.dimension+1)]),['is_anomaly']]\n",
        "\n",
        "\n",
        "\n",
        "        # self.train_size = int(len(self.values) * train_rate)   \n",
        "\n",
        "    def reset_dataframe(self, dataframe, dimension, window_width, train_rate):\n",
        "        self.df = ddataframef\n",
        "        self.dimension = dimension\n",
        "        self.window_width = window_width\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'Target':'is_anomaly'}, inplace=True)\n",
        "        self.df.loc[self.df.is_anomaly == \"'Anomaly'\", 'is_anomaly'] = 1\n",
        "        self.df.loc[self.df.is_anomaly == \"'Normal'\", 'is_anomaly'] = 0\n",
        "\n",
        "        df_sensors = pd.DataFrame(self.df.iloc[:,:dimension].values)  \n",
        "        self.values = df_sensors\n",
        "        self.dataframe = concat([self.df.iloc[:,:-1].shift(1), self.df.iloc[:,:-1], self.df.iloc[:,-1]], axis=1)\n",
        "        self.dataframe.columns = np.r_[np.array(['V'+str(i)+'_t' for i in range(1,self.dimension+1)]),np.array(['V'+str(i)+'_t+1' for i in range(1,self.dimension+1)]),['is_anomaly']]\n",
        "\n",
        "\n",
        "\n",
        "        self.train_size = int(len(self.values) * train_rate)   \n",
        "\n",
        "    def create_XY_lookback_dataset(self,dataset, look_back=1):\n",
        "        dataX, dataY = [], []\n",
        "        for i in range(len(dataset)-look_back-1):\n",
        "            a = dataset[i:(i+look_back),:self.dimension]\n",
        "            dataX.append(a)\n",
        "            dataY.append(dataset[i + look_back,:self.dimension].reshape(-1))\n",
        "        return numpy.array(dataX), numpy.array(dataY)\n",
        "    \n",
        "    def getWindowedVectors(self, X):\n",
        "        vectors = np.zeros((len(X) - self.window_width+1,self.window_width))\n",
        "        for i,_ in enumerate(X[:-self.window_width+1]):\n",
        "            vectors[i] = X[i:i+self.window_width]\n",
        "        return vectors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def build_sets(self):\n",
        "\n",
        "        self.X = self.dataframe.iloc[:,:-1].values\n",
        "        self.Y = self.dataframe.iloc[:,-1].values\n",
        "        self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "        # print('Train.len:',len(self.train),' - Test.len:', len(self.test))\n",
        "\n",
        "        self.train_X, self.train_y = self.create_XY_lookback_dataset(self.train, self.window_width)\n",
        "        self.test_X, self.test_y = self.create_XY_lookback_dataset(self.test, self.window_width)\n",
        "        # print('TrainX.shape:',self.train_X.shape,' - TestX.shape:', self.test_X.shape,' - TrainY.shape:', self.train_y.shape,' - TestY.shape:', self.test_y.shape)\n",
        "\n",
        "        self.validationsize = int(self.train_X.shape[0] * 0.1)\n",
        "        self.val, self.test = self.test[:self.validationsize], self.test[self.validationsize:]\n",
        "        self.val_X, self.val_y= self.test_X[:self.validationsize], self.test_y[:self.validationsize]\n",
        "        self.test_X, self.test_y = self.test_X[self.validationsize:], self.test_y[self.validationsize:]\n",
        "        \n",
        "    def standardize_dataframe(self):\n",
        "\n",
        "        self.scalar = preprocessing.StandardScaler().fit(X)\n",
        "        self.X = self.scalar.transform(X)\n",
        "        self.dataframe = pd.DataFrame(np.concatenate((X,self.dataframe.values[:,-1].reshape(-1,1)),axis=1))\n",
        "        self.dataframe.columns = np.r_[np.array(['V'+str(i)+'_t' for i in range(1,self.dimension+1)]),np.array(['V'+str(i)+'_t+1' for i in range(1,self.dimension+1)]),['is_anomaly']]\n",
        "\n",
        "    def inverse_standardize_dataframe(self):\n",
        "        X = self.dataframe.values[:,:-1]\n",
        "        X = self.scalar.inverse_transform(X)\n",
        "        self.dataframe = pd.DataFrame(np.concatenate((X,self.dataframe.values[:,-1].reshape(-1,1)),axis=1))\n",
        "        self.dataframe.columns = np.r_[np.array(['V'+str(i)+'_t' for i in range(1,self.dimension+1)]),np.array(['V'+str(i)+'_t+1' for i in range(1,self.dimension+1)]),['is_anomaly']]\n",
        "\n",
        "\n",
        "\n",
        "    def model_persistence(self, x):\n",
        "        return x\n",
        "        \n",
        "    def create_persistence(self):\n",
        "        rmse = sqrt(mean_squared_error(self.dataframe.iloc[self.train_size:,:self.dimension], self.dataframe.iloc[self.train_size:,self.dimension:-1]))\n",
        "        # print('Persistent Model RMSE: %.3f' % rmse)   \n",
        "\n",
        "    def fit(self):\n",
        "        self.create_persistence()\n",
        "        self.standardize_dataframe()\n",
        "        self.build_sets()\n",
        "                \n",
        "        self.compute_anomalyScores()\n",
        "        self.inverse_standardize_dataframe()\n",
        "        self.compute_Errors_RMSE()\n",
        "\n",
        "\n",
        "    def plotTraining(self):\n",
        "        history_dict = self.history.history\n",
        "        loss_values = history_dict['loss'][1:]\n",
        "        val_loss_values = history_dict['val_loss'][1:]\n",
        "        self.n_epochs = range(2, self.n_epochs + 1)\n",
        "        plt.plot(self.n_epochs, loss_values, 'bo', label='Training loss')\n",
        "        plt.plot(self.n_epochs, val_loss_values, 'b', label='Validation loss')\n",
        "        plt.title('Training and validation loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def getchange(self, V_0,V_1):\n",
        "        Matrix = V_1.T@V_0@V_0.T@V_1\n",
        "        eigenvalues = np.linalg.eig(Matrix)[0]\n",
        "        lambda_min = np.min(eigenvalues)\n",
        "        return np.sqrt(max((1-lambda_min),0))\n",
        "        \n",
        "    def get_projected_Dataframe(self):\n",
        "        changes_anomaly = np.empty((len(self.X_origin)-self.window_width-1,2))\n",
        "        for i in range(len(self.X_origin)-self.window_width-1):\n",
        "            W_0 = self.X_origin[i:i+self.window_width]\n",
        "            W_1 = self.X_origin[i+1:i+self.window_width+1]\n",
        "            changes_anomaly[i] = [self.getchange(W_0,W_1),self.Y_origin[i]]         \n",
        "        return pd.DataFrame(changes_anomaly)\n",
        "\n",
        "    def compute_anomalyScores(self):\n",
        "\n",
        "        changes_anomaly = np.empty((len(self.train_X)-self.window_width-1,1))\n",
        "        for i in range(len(self.X)-window_width-1):\n",
        "            W_0 = self.X[i:i+window_width]\n",
        "            W_1 = self.X[i+1:i+window_width+1]\n",
        "            changes_anomaly[i] = [getchange(W_0,W_1),self.Y[i]]    \n",
        "\n",
        "        # self.train_X = numpy.reshape(self.train_X, (self.train_X.shape[0],self.dimension,self.train_X.shape[1]))\n",
        "        # self.test_X = numpy.reshape(self.test_X, (self.test_X.shape[0],self.dimension, self.test_X.shape[1]))\n",
        "\n",
        "        from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
        "        self.model = Sequential()\n",
        "\n",
        "        self.model = Sequential()\n",
        "        self.model.add(LSTM(self.n_filters[0], batch_input_shape=(1, self.window_width, self.dimension), stateful=True, return_sequences=True))\n",
        "        self.model.add(LSTM(self.n_filters[1], batch_input_shape=(1, self.window_width, self.dimension), stateful=True))\n",
        "        self.model.add(Dense(self.dimension))\n",
        "        self.model.compile(optimizer='adam', loss='mse')\n",
        "        for i in range(self.n_epochs):\n",
        "            sys.stdout.write('\\r'+str(i)+':'+str(self.n_epochs))\n",
        "            self.model.fit(self.train_X, self.train_y,validation_data=(self.val_X,self.val_y), epochs=1, batch_size=1, verbose=0, shuffle=False)\n",
        "            self.model.reset_states()\n",
        "        print('')\n",
        "\n",
        "        # self.plotTraining()\n",
        "        self.predictions = self.model.predict(self.test_X, batch_size = 1)\n",
        "\n",
        "\n",
        "    def compute_Errors_RMSE(self):\n",
        "\n",
        "        rmse = sqrt(mean_squared_error(self.test_y.reshape(self.predictions.shape), self.predictions))\n",
        "        self.errors = np.absolute(self.test_y.reshape(self.predictions.shape) - np.array(self.predictions))\n",
        "        # print('Prediction Test RMS        E: %.3f' % rmse)       \n",
        "   \n",
        "\n",
        "    def plot(self):\n",
        "        fig, axes = plt.subplots(nrows=3, ncols=3, dpi=120, figsize=(50,5))\n",
        "        for i, ax in enumerate(axes.flatten()):\n",
        "            data = self.df[self.df.columns[i]].iloc[:200]\n",
        "            ax.plot(self.test_y[:,i], color='green',  linewidth=0.5,label='True Values')\n",
        "            ax.plot(self.predictions[:,i], color='blue',  linewidth=0.5,label='Predictions')\n",
        "            ax.plot(self.errors[:,i], color = 'red',  linewidth=0.5, label='Errors')\n",
        "            ax.legend()\n",
        "            \n",
        "        plt.show()\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        self.euclidean_errors = numpy.linalg.norm(self.test_y.reshape(self.predictions.shape) - self.predictions, axis=1)\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size + self.validationsize\n",
        "        true_anomaly_predicted_errors = self.euclidean_errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size - self.validationsize - self.window_width -1]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended)]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.euclidean_errors[self.euclidean_errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.euclidean_errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "\n",
        "# filters = [[8,8,8,8], [4,4,4,4], [4,8,16,32], [16,32,48,64],  [32,64,12,24],[128,128,128,128],[64,64,64,64],[256,256,256,256],[128,256,512,1024]]\n",
        "# kernelsizes = [2,3,4,6,8,16]\n",
        "# dense = [18,36,72,144]\n",
        "\n",
        "# best_auc = [0,0]\n",
        "# histories = []\n",
        "# for f in filters:\n",
        "#     for k in kernelsizes:\n",
        "#         for d in dense:\n",
        "\n",
        "#             cnn = WaveNet_AnomalyDetection('drive/My Drive/MT/Experiments/Multivariate/NASA_Shuttle/40902.csv',30,9,20,0.3,f,k,d)\n",
        "#             hist = cnn.fit()\n",
        "#             histories.append(((f,k,d), hist))\n",
        "#             # cnn.plot()\n",
        "#             auc = cnn.get_roc_auc(verbose=False,plot=False)\n",
        "#             print(' auc:', auc, ' - f:', f, ' - k:', k, ' - d:', d)\n",
        "#             if best_auc[1] < auc:\n",
        "#                 print('New best auc:', auc, ' - f:', f, ' - k:', k, ' - d:', d)\n",
        "#                 best_auc = [(f,k,d),auc]\n",
        "\n",
        "# filters = [[8,8,8,8], [4,4,4,4], [4,8,16,32], [16,32,48,64],  [32,64,12,24],[128,128,128,128],[64,64,64,64],[256,256,256,256],[128,256,512,1024]]\n",
        "# kernelsizes = [2,3,4,6,8,16]\n",
        "# dense = [18,36,72,144]\n",
        "\n",
        "\n",
        "# for epochs in [50,60,70,80,100,150,200]:\n",
        "#     # cnn = LSTM_AnomalyDetection('drive/My Drive/MT/Experiments/Multivariate/NASA_Shuttle/40902.csv',30,9,20,0.3,[4,4],k,d)\n",
        "#     cnn = LSTM_AnomalyDetection(dataframe=df,window_width=13,dimension=5,n_epochs=epochs,train_rate=0.3,n_filters=[7,7])\n",
        "#     # cnn.reset_dataframe(df,5,13, 0.3)\n",
        "#     cnn.fit()\n",
        "#     # # cnn.plot()\n",
        "#     auc = cnn.get_roc_auc(verbose=False,plot=True)\n",
        "#     print(best_auc)\n",
        "#     # 0.59 40 EPochs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7nECiN3iWR_",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfISIFZAiXYC",
        "colab_type": "code",
        "outputId": "fc3af7f1-da02-42d2-b9c7-5237efbad5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "cl = Projection_AnomalyDetection.from_file('drive/My Drive/MT/Experiments/Multivariate/NASA_Shuttle/40903.csv',10,3,0.3)\n",
        "df_projected = cl.get_projected_Dataframe()\n",
        "df_projected.columns = ['V1','is_anomaly']\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0:00:04.948650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1M_xM3Qi-_8",
        "colab_type": "text"
      },
      "source": [
        "### AR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb2_n3fZkBjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "import sys\n",
        "\n",
        "class AR_Compact:\n",
        "\n",
        "    def model_persistence(self, x):\n",
        "        return x\n",
        "\n",
        "    def create_persistence(self):\n",
        "        predictions = list()\n",
        "        for x in self.test_X:\n",
        "            yhat = self.model_persistence(x)\n",
        "            predictions.append(yhat)\n",
        "        rmse = sqrt(mean_squared_error(self.test_y, predictions))\n",
        "        # print('Train shape', self.train_X.shape, ' - Test shape:' , self.test_X.shape)\n",
        "        # print('Persistent Model RMSE: %.3f' % rmse)     \n",
        "        \n",
        "\n",
        "    @classmethod\n",
        "    def from_DataFrame(cls,dataframe, train_rate) -> 'AR_Compact':\n",
        "    \treturn cls(dataframe, train_rate)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, file: str, train_rate) -> 'AR_Compact':\n",
        "    \tdf = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    \treturn cls(df, train_rate)\n",
        "\n",
        "    \n",
        "    def __init__(self,df, train_rate):\n",
        "        self.df = df\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "        self.train_size = int(len(self.values) * train_rate)\n",
        "\n",
        "        self.train, self.test = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = self.train[:,0], self.train[:,1]\n",
        "        self.test_X, self.test_y = self.test[:,0], self.test[:,1]     \n",
        "        self.create_persistence()\n",
        "\n",
        "        # X = (self.dataframe['t+1'] - self.dataframe['t']).values\n",
        "        X = series.values\n",
        "\n",
        "        self.train, self.test = X[1:self.train_size], X[self.train_size:]\n",
        "\n",
        "          \n",
        "    def fit(self, verbose=False):\n",
        "        self.model = AR(self.train)\n",
        "        self.model_fit = self.model.fit()\n",
        "        self.window = self.model_fit.k_ar\n",
        "        self.coef = self.model_fit.params  \n",
        "        if verbose:      \n",
        "            print('Lag: %s' % self.model_fit.k_ar)\n",
        "            print('Coefficients: %s' % self.model_fit.params)\n",
        "\n",
        "    \n",
        "    def predict(self):\n",
        "        self.history = self.train[len(self.train)-self.window:]\n",
        "        self.history = [self.history[i] for i in range(len(self.history))]\n",
        "        self.predictions = list()\n",
        "        for t in range(len(self.test)):\n",
        "            length = len(self.history)\n",
        "            lag = [self.history[i] for i in range(length-self.window,length)]\n",
        "            yhat = self.coef[0]\n",
        "            for d in range(self.window):\n",
        "                yhat += self.coef[d+1] * lag[self.window-d-1]\n",
        "            obs = self.test[t]\n",
        "            self.predictions.append(yhat)\n",
        "            self.history.append(obs)        \n",
        "        # for i in range(len(predictions)):\n",
        "        #     print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
        "        rmse = sqrt(mean_squared_error(self.test, self.predictions))\n",
        "        self.errors = np.absolute(self.test - np.array(self.predictions))\n",
        "        # print('Prediction Test RMSE: %.3f' % rmse)\n",
        "    def plot(self):\n",
        "        # plot predicted error\n",
        "        df_test = pd.DataFrame(self.df.iloc[self.train_size:].values)\n",
        "\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test,color ='blue', linewidth=0.5)\n",
        "        pyplot.plot(self.predictions, color='green',  linewidth=0.5)\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5)\n",
        "        pyplot.plot(df_test[df_test[1]==1.].index, df_test[df_test[1]==1.].iloc[:,0].values,'ro')\n",
        "        pyplot.show()\n",
        "\n",
        "    # def plot(self):\n",
        "    #     # plot predicted error\n",
        "    #     indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "    #     pyplot.figure(figsize=(50,5))\n",
        "    #     pyplot.plot(self.test, color='green',  linewidth=0.5,label='True Values')\n",
        "    #     pyplot.plot(self.predictions, color='blue',  linewidth=0.5,label='Predictions')\n",
        "    #     pyplot.plot(self.errors, color = 'red',  linewidth=0.5, label='Errors')\n",
        "    #     pyplot.plot(self.df[self.df['is_anomaly']==1].index[indices] - self.train_size, self.test[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size -1], linestyle=\"\",marker=\".\", label='Anomalies')\n",
        "    #     pyplot.legend()\n",
        "    #     pyplot.show()\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, max(true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended),np.max(self.errors) + np.mean(self.errors))]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "\n",
        "# ar_model = AR_Compact('drive/My Drive/MT/Experiments/Univariate/YahooServiceNetworkTraffic/A4Benchmark/A4Benchmark-TS18.csv', 0.3)\n",
        "# ar_model.fit()\n",
        "# ar_model.predict()\n",
        "# ar_model.plot()\n",
        "# ar_model.get_roc_auc(verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jdOSnehbAQR",
        "colab_type": "text"
      },
      "source": [
        "#### Results of SMTP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_i5f-XFi_vF",
        "colab_type": "code",
        "outputId": "13c39760-8c7b-4691-cbea-da184a282cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "ar_model = AR_Compact.from_DataFrame(df_projected,0.3)\n",
        "ar_model.fit()\n",
        "ar_model.predict()\n",
        "ar_model.get_roc_auc(verbose=False,plot=False)\n",
        "\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.44328483003513414\n",
            "Time:  0:00:10.263810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzcL5ccojVuF",
        "colab_type": "text"
      },
      "source": [
        "### MA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6TGQMsRjlC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "\n",
        "class MA:\n",
        "\n",
        "    @classmethod\n",
        "    def from_DataFrame(cls,dataframe, train_rate) -> 'MA':\n",
        "    \treturn cls(dataframe, train_rate)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, file: str, train_rate) -> 'MA':\n",
        "    \tdf = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    \treturn cls(df, train_rate)\n",
        "\n",
        "    \n",
        "    def __init__(self,df, train_rate):\n",
        "        self.df = df\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "        X = self.dataframe.values\n",
        "        self.train_size = int(len(X) * train_rate)\n",
        "\n",
        "        self.train, self.test = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = self.train[:,0], self.train[:,1]\n",
        "        self.test_X, self.test_y = self.test[:,0], self.test[:,1]     \n",
        "        # self.create_persistence()\n",
        "\n",
        "        # X = (self.dataframe['t+1'] - self.dataframe['t']).values\n",
        "        X = series.values\n",
        "\n",
        "        # persistence model on training set\n",
        "        self.train_pred = [x for x in self.train_X]\n",
        "        # calculate residuals\n",
        "        self.train_resid = [self.train_y[i]-self.train_pred[i] for i in range(len(self.train_pred))]\n",
        "\n",
        "    # def __init__(self, path, train_rate):\n",
        "\n",
        "    #     self.df = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    #     self.df = self.df.reset_index(drop=True)\n",
        "    #     self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "    #     series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "    #     self.values = DataFrame(series.values)\n",
        "    #     self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "    #     self.dataframe.columns = ['t', 't+1']\n",
        "    #     X = self.dataframe.values\n",
        "\n",
        "    #     self.train_size = int(len(X) * train_rate)    \n",
        "\n",
        "    #     train, test = X[1:self.train_size], X[self.train_size:]\n",
        "    #     self.train_X, self.train_y = train[:,0], train[:,1]\n",
        "    #     self.test_X, self.test_y = test[:,0], test[:,1]        \n",
        "    #     # persistence model on training set\n",
        "    #     self.train_pred = [x for x in self.train_X]\n",
        "    #     # calculate residuals\n",
        "    #     self.train_resid = [self.train_y[i]-self.train_pred[i] for i in range(len(self.train_pred))]\n",
        "\n",
        "    def fit(self, verbose=False):\n",
        "        self.model = AR(self.train_resid)\n",
        "        self.model_fit = self.model.fit()\n",
        "        self.window = self.model_fit.k_ar\n",
        "        self.coef = self.model_fit.params        \n",
        "        if verbose:\n",
        "            print(self.coef)\n",
        "\n",
        "    def predict(self):\n",
        "        # walk forward over time steps in test\n",
        "        self.history = self.train_resid[len(self.train_resid)-self.window:]\n",
        "        self.history = [self.history[i] for i in range(len(self.history))]\n",
        "        self.predictions = list()\n",
        "        for t in range(len(self.test_y)):\n",
        "            # persistence\n",
        "            yhat = self.test_X[t]\n",
        "            error = self.test_y[t] - yhat\n",
        "            # predict error\n",
        "            length = len(self.history)\n",
        "            lag = [self.history[i] for i in range(length-self.window,length)]\n",
        "            pred_error = self.coef[0]\n",
        "            for d in range(self.window):\n",
        "                pred_error += self.coef[d+1] * lag[self.window-d-1]\n",
        "            # correct the prediction\n",
        "            yhat = yhat + pred_error\n",
        "            self.predictions.append(yhat)\n",
        "            self.history.append(error)\n",
        "            # print('predicted=%f, expected=%f' % (yhat, test_y[t]))\n",
        "        rmse = sqrt(mean_squared_error(self.test_y, self.predictions))\n",
        "        self.errors = np.absolute(self.test_y - np.array(self.predictions))\n",
        "        # print('Prediction Test RMSE: %.3f' % rmse)\n",
        "\n",
        "    def plot(self):\n",
        "        # plot predicted error\n",
        "        df_test = pd.DataFrame(self.df.iloc[self.train_size:].values)\n",
        "\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test_y,color ='blue', linewidth=0.5)\n",
        "        pyplot.plot(self.predictions, color='green',  linewidth=0.5)\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5)\n",
        "        pyplot.plot(df_test[df_test[1]==1.].index, df_test[df_test[1]==1.].iloc[:,0].values,'ro')\n",
        "        pyplot.show()\n",
        "\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size ]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index - self.train_size ]\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended)]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "\n",
        "# ma_model = MA('drive/My Drive/MT/Experiments/Univariate/YahooServiceNetworkTraffic/A1Benchmark/real_14.csv',0.66)\n",
        "# ma_model.fit()\n",
        "# ma_model.predict()\n",
        "# ma_model.plot()\n",
        "# ma_model.get_roc_auc(verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGlACb3bbHMU",
        "colab_type": "text"
      },
      "source": [
        "#### Results of SMTP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSWGndaujXeE",
        "colab_type": "code",
        "outputId": "5d394d71-2438-43ed-b2a9-26bc205dba07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "ma = MA.from_DataFrame(df_projected,0.3)\n",
        "ma.fit()\n",
        "ma.predict()\n",
        "ma.get_roc_auc(verbose=False,plot=False)\n",
        "\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.587288576168884\n",
            "Time:  0:00:02.163155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bEZU9KnkeZk",
        "colab_type": "text"
      },
      "source": [
        "### ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b75Om_z_kfXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "class ARIMA_Compact:\n",
        "\n",
        "    def model_persistence(self, x):\n",
        "        return x\n",
        "\n",
        "    def create_persistence(self):\n",
        "        predictions = list()\n",
        "        for x in self.test_X:\n",
        "            yhat = self.model_persistence(x)\n",
        "            predictions.append(yhat)\n",
        "        rmse = sqrt(mean_squared_error(self.test_y, predictions))\n",
        "        # print('Train shape', self.train_X.shape, ' - Test shape:' , self.test_X.shape)\n",
        "        # print('Persistent Model RMSE: %.3f' % rmse)        \n",
        "\n",
        "    @classmethod\n",
        "    def from_DataFrame(cls,dataframe, train_rate,order) -> 'ARIMA_Compact':\n",
        "    \treturn cls(dataframe, train_rate,order)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, file: str, train_rate,order) -> 'ARIMA_Compact':\n",
        "    \tdf = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "    \treturn cls(df, train_rate,order)\n",
        "     \n",
        "    def __init__(self, df, train_rate, order):\n",
        "        \n",
        "        self.order = order\n",
        "        self.df = df\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\t\t\n",
        "\n",
        "        self.train_size = int(len(self.values) * train_rate)\n",
        "\n",
        "        self.train, self.test = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = self.train[:,0], self.train[:,1]\n",
        "        self.test_X, self.test_y = self.test[:,0], self.test[:,1]     \n",
        "        self.create_persistence()\n",
        "\n",
        "        X = series.values\n",
        "        self.train, self.test = X[1:self.train_size], X[self.train_size:]\n",
        "        self.train = self.train.astype('float32')\n",
        "        self.test = self.test.astype('float32')\n",
        "          \n",
        "    def fit(self,  verbose=False):\n",
        "        self.model = ARIMA(self.train, order = self.order)\n",
        "        self.model_fit = self.model.fit(disp=0)\n",
        "\n",
        "    \n",
        "    def predict(self):\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        self.history = [x for x in self.train]\n",
        "        self.predictions = list()\n",
        "        for t in range(len(self.test)):\n",
        "            self.model = ARIMA(self.history,order=self.order)\n",
        "            self.model = self.model.fit(disp=0)\n",
        "            output = self.model.forecast()\n",
        "            yhat = output[0]\n",
        "            self.predictions.append(yhat)\n",
        "            obs = self.test[t]\n",
        "            self.history.append(obs)\n",
        "            sys.stdout.write('\\r'+str(t)+':'+str(len(self.test)))\n",
        "        print('')    \n",
        "\t\t\t\n",
        "        # for i in range(len(predictions)):\n",
        "        #     print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
        "        rmse = sqrt(mean_squared_error(self.test, self.predictions))\n",
        "        self.errors = np.absolute(self.test - np.array(self.predictions))\n",
        "        # print('Prediction Test RMSE: %.3f' % rmse)\n",
        "\n",
        "    def plot(self):\n",
        "        # plot predicted error\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test_y, color='green',  linewidth=0.5,label='True Values')\n",
        "        pyplot.plot(self.predictions, color='blue',  linewidth=0.5,label='Predictions')\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5, label='Errors')\n",
        "        pyplot.legend()\n",
        "        pyplot.show()\n",
        "\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size ]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, max(true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended),np.max(self.errors) + np.mean(self.errors))]\n",
        "        # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        # print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "\n",
        "# arma_model = ARIMA_Compact('drive/My Drive/MT/Experiments/Univariate/YahooServiceNetworkTraffic/A1Benchmark/real_1.csv', 0.66, (0,0,2))\n",
        "# arma_model.fit()\n",
        "# arma_model.predict()\n",
        "# arma_model.plot()\n",
        "# arma_model.get_roc_auc(verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-MJd47ybJch",
        "colab_type": "text"
      },
      "source": [
        "#### Results of SMTP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cInQI9h8kf7D",
        "colab_type": "code",
        "outputId": "3eb66474-95e2-420f-99cc-c7bc15c2bcd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import datetime\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "arima_model = ARIMA_Compact.from_DataFrame(df_projected,0.3, (1,0,2))\n",
        "arima_model.fit()\n",
        "arima_model.predict()\n",
        "auc = arima_model.get_roc_auc(verbose=False)\n",
        "print(auc)\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff, 'ms')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2819:66602"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}