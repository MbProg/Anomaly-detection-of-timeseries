{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnivariateML NYCT (OCSVM+XGBoost).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f790rydIve_",
        "colab_type": "code",
        "outputId": "4d6b8a6b-d93b-4c67-a2a4-2de8cfe08eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMZJzz67FLEt",
        "colab_type": "text"
      },
      "source": [
        "# Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzcloO8dFIdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getScaledTrainTextDataset(dataset_scaled, trainRate=0.3):\n",
        "    print('Shape: ',dataset_scaled.shape)\n",
        "    train_size = int(len(dataset_scaled)*trainRate)\n",
        "    test_size = len(dataset_scaled) - train_size\n",
        "    print('Trainsize:',train_size, ' - Testsize:',test_size)\n",
        "    train_start_index,train_end_index = 0,train_size\n",
        "    test_start_index,test_end_index = train_size,len(dataset_scaled)\n",
        "    train = dataset_scaled[0:train_size]\n",
        "    test = dataset_scaled[train_size:] \n",
        "    return train,test\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_XY_lookback_dataset(dataset, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back)]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back])\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "def create_XY_lookback_dataset_multistepOutput(dataset, look_back=1, look_forward = 1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back-1-look_forward):\n",
        "\t\ta = dataset[i:(i+look_back), 0]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back:i+look_back+look_forward, 0])\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "def calculateRMSE(testPredict,trainY,testY,inverse_transform=True, verbose= 1):\n",
        "    \n",
        "    if inverse_transform:\n",
        "        testPredict_inverse = scaler.inverse_transform(testPredict)\n",
        "        testY_inverse = scaler.inverse_transform([testY])  \n",
        "    else:\n",
        "        testPredict_inverse = testPredict\n",
        "        testY_inverse = testY\n",
        "        \n",
        "    # calculate root mean squared error\n",
        "    testScore = math.sqrt(mean_squared_error(testY_inverse[0], testPredict_inverse[:,0]))\n",
        "    if verbose == 1:\n",
        "        print('Test Score: %.2f RMSE' % (testScore))\n",
        "        print('Persistent Model Testscore small:',global_testPredict_small, ' - Persistent Model Testscore big:', global_testPredict_big)\n",
        "        \n",
        "    return  testScore\n",
        "\n",
        "def calculateRMSE_MultipleOutput(trainPredict,testPredict,trainY,testY,inverse_transform=True, verbose= 1):\n",
        "    \n",
        "    if inverse_transform:\n",
        "        trainPredict_inverse = scaler.inverse_transform(trainPredict)\n",
        "        trainY_inverse = scaler.inverse_transform(trainY)\n",
        "        testPredict_inverse = scaler.inverse_transform(testPredict)\n",
        "        testY_inverse = scaler.inverse_transform(testY)  \n",
        "    else:\n",
        "        trainPredict_inverse = trainPredict\n",
        "        trainY_inverse = trainY\n",
        "        testPredict_inverse = testPredict\n",
        "        testY_inverse = testY\n",
        "        \n",
        "    # calculate root mean squared error\n",
        "    trainScore = math.sqrt(mean_squared_error(trainY_inverse[0], trainPredict_inverse[:,0]))\n",
        "    testScore = math.sqrt(mean_squared_error(testY_inverse[0], testPredict_inverse[:,0]))\n",
        "    if verbose == 1:\n",
        "        print('Train Score: %.2f RMSE' % (trainScore))\n",
        "        print('Test Score: %.2f RMSE' % (testScore))\n",
        "        print('Persistent Model Testscore small:',global_testPredict_small, ' - Persistent Model Testscore big:', global_testPredict_big)\n",
        "        \n",
        "    return trainScore, testScore\n",
        "\n",
        "def plotErrorPrediction(dataset_scaled, testPredict, trainPredict, ShowTestError=True, inverse_transform=True):\n",
        "    if inverse_transform:\n",
        "        trainPredict_inverse = scaler.inverse_transform(trainPredict)\n",
        "        testPredict_inverse = scaler.inverse_transform(testPredict)\n",
        "    else:\n",
        "        trainPredict_inverse = trainPredict\n",
        "        testPredict_inverse = testPredict\n",
        "\n",
        "    # shift train predictions for plotting\n",
        "    trainPredictPlot = numpy.zeros_like(dataset_scaled)\n",
        "    trainPredictPlot[:, :] = numpy.nan\n",
        "    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict_inverse\n",
        "    # shift test predictions for plotting\n",
        "    testPredictPlot = numpy.empty_like(dataset_scaled)\n",
        "    testPredictPlot[:, :] = numpy.nan\n",
        "    testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict_inverse    \n",
        "    \n",
        "    error_test = dataset-testPredictPlot\n",
        "    error_test[np.isnan(error_test)] = 0.\n",
        "    # error_test[error_training<2000] = 0.\n",
        "    error_test = np.abs(error_test)\n",
        "    import seaborn as sns\n",
        "    sns.distplot(error_test[error_test!=0])\n",
        "    print(pandas.DataFrame(error_test[error_test!=0]).describe())\n",
        "    return error_test, trainPredictPlot,testPredictPlot\n",
        "\n",
        "def plotErrorPredictionValidation(dataset_scaled, validationPredict, testPredict, trainPredict, ShowTestError=True, inverse_transform=True):\n",
        "    if inverse_transform:\n",
        "        trainPredict_inverse = scaler.inverse_transform(trainPredict)\n",
        "        testPredict_inverse = scaler.inverse_transform(testPredict)\n",
        "    else:\n",
        "        trainPredict_inverse = trainPredict\n",
        "        testPredict_inverse = testPredict\n",
        "\n",
        "    # shift train predictions for plotting\n",
        "    trainPredictPlot = numpy.zeros_like(dataset_scaled)\n",
        "    trainPredictPlot[:, :] = numpy.nan\n",
        "    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict_inverse\n",
        "    # shift test predictions for plotting\n",
        "    testPredictPlot = numpy.empty_like(dataset_scaled)\n",
        "    testPredictPlot[:, :] = numpy.nan\n",
        "    testPredictPlot[len(trainPredict)+(look_back*2)+1+len(validationPredict):len(dataset)-1, :] = testPredict_inverse    \n",
        "    \n",
        "    error_test = dataset-testPredictPlot\n",
        "    error_test[np.isnan(error_test)] = 0.\n",
        "    # error_test[error_training<2000] = 0.\n",
        "    error_test = np.abs(error_test)\n",
        "    import seaborn as sns\n",
        "    sns.distplot(error_test[error_test!=0])\n",
        "    print(pandas.DataFrame(error_test[error_test!=0]).describe())\n",
        "    return error_test, trainPredictPlot,testPredictPlot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ro4B1PETxo",
        "colab_type": "text"
      },
      "source": [
        "# OCSVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHzUiqUsEU7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "\n",
        "class OneClassSVM_AnomalyDetection:\n",
        "    def __init__(self,path, window_width, nu, train_rate):\n",
        "        self.df = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "        self.nu = nu\n",
        "        self.window_width = window_width\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "        self.train_size = int(len(self.values) * train_rate)\n",
        "\n",
        "        # train_labeled, test_labeled = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        # self.train_X, self.train_y = train_labeled[:,0], train_labeled[:,1]\n",
        "        # self.test_X, self.test_y = test_labeled[:,0], test_labeled[:,1]     \n",
        "        # self.create_persistence()\n",
        "\n",
        "        # X = series.values\n",
        "        # self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "\n",
        "    def __build_sets(self):\n",
        "        train_labeled, test_labeled = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = train_labeled[:,0], train_labeled[:,1]\n",
        "        self.test_X, self.test_y = test_labeled[:,0], test_labeled[:,1]   \n",
        "\n",
        "        X = self.dataframe.iloc[:,1].values\n",
        "        self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "\n",
        "    def standardize_dataframe(self):\n",
        "        X = self.dataframe.values\n",
        "        self.scalar = preprocessing.StandardScaler().fit(X)\n",
        "        X = self.scalar.transform(X)\n",
        "        self.dataframe = pd.DataFrame(X)\n",
        "\n",
        "    def inverse_standardize_dataframe(self):\n",
        "        X = self.dataframe.values\n",
        "        X = self.scalar.inverse_transform(X)\n",
        "        self.dataframe = pd.DataFrame(X)\n",
        "\n",
        "\n",
        "\n",
        "    def model_persistence(self, x):\n",
        "        return x\n",
        "        \n",
        "    def create_persistence(self):\n",
        "        rmse = sqrt(mean_squared_error(self.dataframe['t'].iloc[self.train_size:], self.dataframe['t+1'].iloc[self.train_size::]))\n",
        "#         print('Persistent Model RMSE: %.3f' % rmse)   \n",
        "\n",
        "    def fit(self):\n",
        "        self.create_persistence()\n",
        "        self.standardize_dataframe()\n",
        "        self.__build_sets()\n",
        "                \n",
        "        self.compute_anomalyScores()\n",
        "        self.inverse_standardize_dataframe()\n",
        "    \n",
        "    def getWindowedVectors(self, X):\n",
        "        vectors = []\n",
        "        for i,_ in enumerate(X[:-self.window_width+1]):\n",
        "            vectors.append(X[i:i+self.window_width])\n",
        "        return vectors\n",
        "\n",
        "    def compute_anomalyScores(self):\n",
        "        self.errors = np.zeros_like(self.test)\n",
        "        # compute anomalies\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "        # history = self.getWindowedVectors(self.train)\n",
        "\n",
        "        for i,_ in enumerate(self.test[:-self.window_width+1]):\n",
        "            sys.stdout.write('\\r'+str(i)+':'+str(len(self.test) - self.window_width))\n",
        "\n",
        "            window = self.test[i:i+self.window_width]\n",
        "            window2D = np.zeros((len(window),2))\n",
        "            window2D[:,1] = window\n",
        "            clf=OneClassSVM(nu=self.nu)\n",
        "            clf.fit(window2D)\n",
        "            error = clf.decision_function(window2D) \n",
        "            error[error>0] = 0\n",
        "            self.errors[i:i+self.window_width] += error*-10\n",
        "\n",
        "\n",
        "        # normalize anomaly score\n",
        "        self.errors[:-self.window_width+1] /= self.window_width\n",
        "        for i,error in enumerate(self.test[-self.window_width+1:]):\n",
        "            self.errors[-self.window_width + 1 + i] /=self.window_width-(i+1)\n",
        "\n",
        "        # self.errors_original = self.errors\n",
        "        # scalar = preprocessing.MinMaxScaler((0,1)).fit(self.errors.reshape(-1,1))\n",
        "        # self.errors = scalar.transform(self.errors.reshape(-1,1))*10\n",
        "\n",
        "\n",
        "    def plot(self):\n",
        "        # plot predicted error\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test)\n",
        "        # pyplot.plot(self.predictions, color='red')\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5)\n",
        "        pyplot.show()\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size ]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended)]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n",
        "    \n",
        "# iforest = OneClassSVM_AnomalyDetection('Univariate/YahooServiceNetworkTraffic/A1Benchmark/real_1.csv',30,0.7,0.3)\n",
        "# iforest.fit()\n",
        "# iforest.plot()\n",
        "# iforest.get_roc_auc(verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isHOKP0zEZMt",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro_D3Gve-Zet",
        "colab_type": "text"
      },
      "source": [
        "### Results of NYCT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTct8QU0EaJi",
        "colab_type": "code",
        "outputId": "729ce856-7ad8-4dac-c6d6-317dbfd06c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "import glob\n",
        "\n",
        "\n",
        "cnn = OneClassSVM_AnomalyDetection( 'drive/My Drive/MT/Experiments/Univariate/NYC_Taxi/nyc_taxi.csv', 40,0.1,0.3)\n",
        "cnn.fit()\n",
        "cnn.get_roc_auc(plot=False,verbose=False)\n",
        "\n",
        "        \n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff)\n",
        "cnn = OneClassSVM_AnomalyDetection( 'drive/My Drive/MT/Experiments/Univariate/NYC_Taxi/nyc_taxi.csv', 40,0.1,0.3)\n",
        "\n",
        "startTime = datetime.datetime.now()\n",
        "cnn.fit()       \n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Inference Time: ',diff)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7184:7184AUC:  0.5210562102565175\n",
            "Time:  0:00:06.942960\n",
            "7184:7184Inference Time:  0:00:07.040107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWmyvSd6GryL",
        "colab_type": "text"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqNLghKQGtxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "import numpy as np \n",
        "from matplotlib import pyplot\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "\n",
        "class XGBRegressor_AnomalyDetection:\n",
        "    def __init__(self,path, window_width, nu, train_rate):\n",
        "        self.df = read_csv(path, header=0, index_col=0, parse_dates=True,squeeze=True)\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        self.df.rename(columns={'anomaly':'is_anomaly'}, inplace=True)\n",
        "        self.nu = nu\n",
        "        self.window_width = window_width\n",
        "        series = pd.DataFrame(self.df.iloc[:,0].values)  \n",
        "        self.values = DataFrame(series.values)\n",
        "        self.dataframe = concat([self.values.shift(1), self.values], axis=1)\n",
        "        self.dataframe.columns = ['t', 't+1']\n",
        "\n",
        "        self.train_size = int(len(self.values) * train_rate)\n",
        "\n",
        "        # train_labeled, test_labeled = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        # self.train_X, self.train_y = train_labeled[:,0], train_labeled[:,1]\n",
        "        # self.test_X, self.test_y = test_labeled[:,0], test_labeled[:,1]     \n",
        "        # self.create_persistence()\n",
        "\n",
        "        # X = series.values\n",
        "        # self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "\n",
        "    def __build_sets(self):\n",
        "        train_labeled, test_labeled = self.dataframe.values[1:self.train_size], self.dataframe.values[self.train_size:]\n",
        "        self.train_X, self.train_y = train_labeled[:,0], train_labeled[:,1]\n",
        "        self.test_X, self.test_y = test_labeled[:,0], test_labeled[:,1]   \n",
        "\n",
        "        X = self.dataframe.iloc[:,1].values\n",
        "        self.train, self.test = X[1:self.train_size], X[self.train_size:]    \n",
        "\n",
        "    def standardize_dataframe(self):\n",
        "        X = self.dataframe.values\n",
        "        self.scalar = preprocessing.StandardScaler().fit(X)\n",
        "        X = self.scalar.transform(X)\n",
        "        self.dataframe = pd.DataFrame(X)\n",
        "\n",
        "    def inverse_standardize_dataframe(self):\n",
        "        X = self.dataframe.values\n",
        "        X = self.scalar.inverse_transform(X)\n",
        "        self.dataframe = pd.DataFrame(X)\n",
        "\n",
        "\n",
        "\n",
        "    def model_persistence(self, x):\n",
        "        return x\n",
        "        \n",
        "    def create_persistence(self):\n",
        "        rmse = sqrt(mean_squared_error(self.dataframe['t'].iloc[self.train_size:], self.dataframe['t+1'].iloc[self.train_size::]))\n",
        "#         print('Persistent Model RMSE: %.3f' % rmse)   \n",
        "\n",
        "    def fit(self):\n",
        "        self.create_persistence()\n",
        "        self.__build_sets()\n",
        "                \n",
        "        self.compute_anomalyScores()\n",
        "    \n",
        "    def getWindowedVectors(self, X):\n",
        "        vectors = []\n",
        "        for i,_ in enumerate(X[:-self.window_width+1]):\n",
        "            vectors.append(X[i:i+self.window_width])\n",
        "        return vectors\n",
        "\n",
        "    def compute_anomalyScores(self):\n",
        "\n",
        "        self.xgb = XGBRegressor()\n",
        "        self.xgb.fit(self.train_X.reshape(-1,1),self.train_y.reshape(-1,1))\n",
        "\n",
        "        self.predictions = self.xgb.predict(self.test_X.reshape(-1,1))\n",
        "        rmse = sqrt(mean_squared_error(self.test, self.predictions))\n",
        "        self.errors = np.absolute(self.test - np.array(self.predictions))\n",
        "#         print('Prediction Test RMSE: %.3f' % rmse)\n",
        "    \n",
        "\n",
        "    def plot(self):\n",
        "        # plot predicted error\n",
        "        pyplot.figure(figsize=(50,5))\n",
        "        pyplot.plot(self.test)\n",
        "        pyplot.plot(self.predictions, color='blue')\n",
        "        pyplot.plot(self.errors, color = 'red',  linewidth=0.5)\n",
        "        pyplot.show()\n",
        "\n",
        "    def get_roc_auc(self, plot=True, verbose=True):\n",
        "        # get the predicted errors of the anomaly points\n",
        "        indices = self.df[self.df['is_anomaly']==1].index >self.train_size\n",
        "        true_anomaly_predicted_errors = self.errors[self.df[self.df['is_anomaly']==1].index[indices] - self.train_size ]\n",
        "        if len(true_anomaly_predicted_errors) == 0:\n",
        "            return np.nan\n",
        "        # sort them \n",
        "        true_anomaly_predicted_errors = np.sort(true_anomaly_predicted_errors,axis=0).reshape(-1)\n",
        "        true_anomaly_predicted_errors_extended = np.r_[np.linspace(0,true_anomaly_predicted_errors[0],40)[:-1],true_anomaly_predicted_errors]\n",
        "        true_anomaly_predicted_errors_extended = np.r_[true_anomaly_predicted_errors_extended, true_anomaly_predicted_errors_extended[-1] + np.mean(true_anomaly_predicted_errors_extended)]\n",
        "                # now iterate thru the predicted errors from small to big\n",
        "        # for each value look how much other points have equal or bigger error\n",
        "        FPR = [] # fp/n  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "        TPR = [] # tp/p\n",
        "        p = len(true_anomaly_predicted_errors)\n",
        "        Thresholds = []\n",
        "        for predictederror in true_anomaly_predicted_errors_extended:\n",
        "            threshold = predictederror\n",
        "            tp = len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>= threshold])\n",
        "            fp = len(self.errors[self.errors>=threshold])-len(true_anomaly_predicted_errors[true_anomaly_predicted_errors>=threshold])\n",
        "            \n",
        "            fpr =fp/len(self.errors)\n",
        "            FPR.append(fpr)\n",
        "            TPR.append(tp/p)\n",
        "            if verbose:\n",
        "                print(\"Threshold: {0:25}  - FP: {1:4} - TP: {2:4} - FPR: {3:21} - TPR: {4:4}\".format(threshold,fp, tp, fpr, tp/p))\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        if plot:\n",
        "            plt.figure()\n",
        "            plt.axis([0, 1, 0, 1])\n",
        "            plt.plot(FPR,TPR)\n",
        "            plt.show() \n",
        "\n",
        "        # This is the AUC\n",
        "        from sklearn.metrics import auc\n",
        "        print('AUC: ' ,auc(FPR,TPR)        )\n",
        "        return auc(FPR,TPR)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wWonu_M-dSU",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBay2P7A-c3d",
        "colab_type": "text"
      },
      "source": [
        "### Results of NYCT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OzOzUqPGzgg",
        "colab_type": "code",
        "outputId": "7fae9c2f-28ae-4964-ac5f-40747c304148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "startTime = datetime.datetime.now()\n",
        "\n",
        "import glob\n",
        "\n",
        "\n",
        "cnn = XGBRegressor_AnomalyDetection( 'drive/My Drive/MT/Experiments/Univariate/NYC_Taxi/nyc_taxi.csv', 1000,0.5,0.3)\n",
        "cnn.fit()\n",
        "cnn.get_roc_auc(plot=False,verbose=False)\n",
        "\n",
        "        \n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Time: ',diff)\n",
        "\n",
        "startTime = datetime.datetime.now()\n",
        "cnn.xgb.predict(cnn.test_X.reshape(-1,1))\n",
        "endTime = datetime.datetime.now()\n",
        "diff = endTime - startTime\n",
        "print('Inference Time: ',diff)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[03:44:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "AUC:  0.4602617410866643\n",
            "Time:  0:00:00.139282\n",
            "Inference Time:  0:00:00.020563\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}